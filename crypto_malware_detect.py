# -*- coding: utf-8 -*-
"""crypto_malware_detect

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TWkTFpUC7PUxf83msAi2-ZxXFj-RP16J
"""

!pip install pandas numpy matplotlib seaborn

!pip install scikit-learn imbalanced-learn joblib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import SMOTE
from google.colab import files
import joblib

uploaded = files.upload()

# Read dataset
df = pd.read_csv('dataset_malwares.csv')
df.head()

print("Dataset Information:")
print(df.info())

print("Missing values per column:")
print(df.isnull().sum())

df.dropna(inplace=True)

# Display label distribution before encoding
print("Label distribution before encoding:")

# Add this line to print the column names
print("Columns in DataFrame:", df.columns)

# Check if 'Malware' is in the columns before trying to access it
if 'Malware' in df.columns:
    print(df['Malware'].value_counts())
else:
    # This case should ideally not be reached given the previous output,
    # but it's good practice to keep a check for robustness.
    print("Error: 'Malware' column not found in the DataFrame.")

le = LabelEncoder()
df['Malware'] = le.fit_transform(df['Malware'])

X = df.drop('Malware', axis=1)
y = df['Malware']

scaler = StandardScaler()

# Select only numeric columns from X before scaling
X_numeric = X.select_dtypes(include=np.number)

# Fit and transform the scaled data using only the numeric columns
X_scaled = scaler.fit_transform(X_numeric)

# You might want to keep track of the column names for later
numeric_cols = X_numeric.columns

# Step 12: Feature Selection with ANOVA F-Test

# Initialize SelectKBest to select the top k features based on ANOVA F-value
# You can choose the number of features (k) based on your needs,
# or use 'all' to see scores for all features.
k_best = SelectKBest(score_func=f_classif, k=50) # Example: selecting top 50 features

# Fit SelectKBest to the scaled numeric features and the target variable
# Note: f_classif is designed for classification tasks and works with numerical features (X)
# and a categorical/integer target variable (y).
# We use X_numeric here because SelectKBest, like StandardScaler, works best with numerical input features.
k_best.fit(X_numeric, y)

# Get the indices of the selected features
selected_features_indices = k_best.get_support(indices=True)

# Get the names of the selected features from the list of numeric column names
selected_features_names = numeric_cols[selected_features_indices]

# Get the scores for all features
feature_scores = k_best.scores_

# Create a DataFrame to display feature names and their scores
scores_df = pd.DataFrame({'Feature': numeric_cols, 'Score': feature_scores})

# Sort the features by score in descending order
scores_df = scores_df.sort_values(by='Score', ascending=False)

# Print the selected features and their scores
print("Top features selected by ANOVA F-test:")
print(scores_df.head(len(selected_features_names))) # Display details for selected features

# Transform the feature set X_numeric to include only the selected features
X_selected = k_best.transform(X_numeric)

print(f"\nOriginal number of numeric features: {X_numeric.shape[1]}")
print(f"Number of features after selection: {X_selected.shape[1]}")

# Step 13: Display Selected Feature Indices

# Print the indices of the selected features.
# These indices correspond to the positions of the selected features
# in the X_numeric DataFrame (or the original numeric_cols list).
print("Indices of selected features:")
print(selected_features_indices)

# Step 14: Correlation Heatmap

# Calculate the correlation matrix for the selected features
# We use the original numeric features X_numeric, but filter to include only selected columns
X_selected_df = X_numeric[selected_features_names]
correlation_matrix = X_selected_df.corr()

# Set up the matplotlib figure
plt.figure(figsize=(15, 12)) # Adjust figure size as needed for readability

# Create the heatmap using seaborn
# annot=False to avoid overcrowding the heatmap with values, especially for many features
# cmap can be changed to a different color map (e.g., 'coolwarm', 'viridis', 'plasma')
sns.heatmap(correlation_matrix, annot=False, cmap='viridis')

# Add title to the heatmap
plt.title('Correlation Heatmap of Selected Features', fontsize=18)

# Rotate the x-axis labels for better readability if needed
plt.xticks(rotation=90)
plt.yticks(rotation=0)

# Ensure the layout is tight to prevent labels from overlapping
plt.tight_layout()

# Display the heatmap
plt.show()

# Step 15: Plot Class Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x=y)
plt.title('Distribution of Malware Classes')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks([0, 1], ['Benign', 'Malware'])
plt.show()

X_entropy = np.random.random(size=(X_selected.shape[0], 1))
X_selected = np.hstack((X_selected, X_entropy))

# Step 17: Split Data and Handle Imbalance (using SMOTE)

# Split the data into training and testing sets
# test_size: the proportion of the dataset to include in the test split
# random_state: ensures reproducibility of the split
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.25, random_state=42, stratify=y)

print(f"Original training set size: {X_train.shape[0]}")
print(f"Original testing set size: {X_test.shape[0]}")
print(f"Class distribution in y_train before SMOTE:\n{pd.Series(y_train).value_counts()}")

# Apply SMOTE to the training data to handle potential class imbalance
# SMOTE creates synthetic samples of the minority class.
# It should ONLY be applied to the training data to avoid data leakage.
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print(f"\nTraining set size after SMOTE: {X_train_res.shape[0]}")
print(f"Class distribution in y_train after SMOTE:\n{pd.Series(y_train_res).value_counts()}")

# The test set remains unchanged
print(f"Testing set size remains: {X_test.shape[0]}")

sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_selected, y)

# Step 18: Show Balanced Class Distribution

# Plot the class distribution after applying SMOTE
plt.figure(figsize=(6, 4))
sns.countplot(x=y_train_res)
plt.title('Distribution of Malware Classes (After SMOTE on Training Data)')
plt.xlabel('Class')
plt.ylabel('Count')
# Assuming the encoded classes are 0 and 1, and 1 represents Malware
plt.xticks([0, 1], ['Benign', 'Malware'])
plt.show()

# Print the value counts for confirmation
print("Class distribution in y_train_res after SMOTE:")
print(pd.Series(y_train_res).value_counts())

X_train, X_test, y_train, y_test = train_test_split(
    X_res, y_res, test_size=0.2, stratify=y_res, random_state=42
)

# Step 19: Train a Classifier Model

# Initialize the Random Forest Classifier
# You can adjust parameters like n_estimators (number of trees) and random_state
model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores

# Train the model using the SMOTE-resampled training data
print("Training the Random Forest model...")
model.fit(X_train_res, y_train_res)
print("Model training complete.")

# Step 20: Evaluate the Model (Revised)

# Make predictions on the test set
# We use the original, non-SMOTE test data for evaluation
# Use the 'model' variable which holds the trained classifier
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1] # Get probability of the positive class (Malware)

# You can then proceed with evaluation metrics like classification_report, etc.
# The code for evaluation metrics was provided in the previous response.

from sklearn.metrics import classification_report

print("Classification Report:")
# Add target_names for better readability of the report
print(classification_report(y_test, y_pred, target_names=['Benign', 'Malware']))

# Calculate ROC AUC Score

from sklearn.metrics import roc_auc_score

# Ensure y_proba was calculated in the previous step
# If not, you need to make predictions with probabilities first:
# y_proba = model.predict_proba(X_test)[:, 1]

roc_auc = roc_auc_score(y_test, y_proba)
print(f"\nROC AUC Score: {roc_auc:.4f}")

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
# Plot the confusion matrix as a heatmap for better visualization
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malware'], yticklabels=['Benign', 'Malware'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

from sklearn.metrics import RocCurveDisplay
import matplotlib.pyplot as plt

# Step 25: Plot ROC Curve

# Use the 'model' variable which holds the trained classifier
plt.figure(figsize=(8, 6)) # Set figure size for better visibility
RocCurveDisplay.from_estimator(model, X_test, y_test)

plt.title('ROC Curve')
plt.xlabel('False Positive Rate') # Add axis labels for clarity
plt.ylabel('True Positive Rate')
plt.plot([0, 1], [0, 1], 'k--', label='Random') # Plot random guess line
plt.legend() # Show legend for the random line
plt.show()

# Step 26: Show Feature Importances

# Get feature importances from the trained model
# Use 'model' instead of 'clf'
importances = model.feature_importances_

# Get the names of the features used for training
# Remember that X_selected included the original selected numeric features
# and the added 'X_entropy' column.
# We need to reconstruct the list of feature names accordingly.
# Assuming the last feature added was the 'entropy' placeholder.
feature_names_for_importance = list(selected_features_names) + ['Entropy_Placeholder'] # Adjust name if needed

# Create a DataFrame of feature importances
feature_importance_df = pd.DataFrame({
    'Feature': feature_names_for_importance,
    'Importance': importances
})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

print("Feature Importances (Top Features):")
# Print the entire sorted DataFrame or just the top N features
print(feature_importance_df)

# Optional: Plot the top N feature importances for better visualization
plt.figure(figsize=(10, len(feature_importance_df) * 0.3)) # Adjust figure size based on number of features
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importances')
plt.tight_layout()
plt.show()

# Step 27: Save Trained Model

import joblib

# Save the trained model to a .pkl file
# Use 'model' instead of 'clf'
joblib.dump(model, "malware_model.pkl")

print("Model saved successfully as 'malware_model.pkl'")

# Save the fitted scaler to a .pkl file
joblib.dump(scaler, "scaler.pkl")

print("Scaler saved successfully as 'scaler.pkl'")

import numpy as np

def predict(sample):
    sample_reshaped = np.array(sample).reshape(1, -1)
    prediction = model.predict(sample_reshaped)
    return prediction[0]

# Step 30: Load Model and Scaler and Make a Prediction

import joblib
import numpy as np

# Load the trained model
try:
    loaded_model = joblib.load("malware_model.pkl")
    print("Model loaded successfully.")
except FileNotFoundError:
    print("Error: 'malware_model.pkl' not found. Please ensure you saved the model.")
    loaded_model = None

# Load the fitted scaler
try:
    loaded_scaler = joblib.load("scaler.pkl")
    print("Scaler loaded successfully.")
except FileNotFoundError:
    print("Error: 'scaler.pkl' not found. Please ensure you saved the scaler.")
    loaded_scaler = None

# Define the prediction function (can be the same as before, but using loaded objects)
# NOTE: This function needs to be consistent with the preprocessing applied BEFORE
# the model received data during training. Based on your training, the model was likely
# trained on non-scaled selected numeric features + entropy.
# So, if you reload the scaler, it's for demonstrating its loading, but the
# inference function itself *should not* use the scaler if training was on non-scaled data.

# Redefining the predict function using the loaded model, and assuming input
# needs to be processed *before* prediction to match the trained model's input format.
# This is still based on the assumption that the model expects features
# in the format of X_selected_with_entropy (selected numeric + entropy).

def predict_loaded_model(sample, model_to_use):
    # Assumes 'sample' is the raw input, and you need to apply
    # feature selection and entropy adding logic here. This is complex.

    # STICKING TO THE SIMPLER INFERENCE FUNCTION (assuming input is already in the correct format):
    # Using the 'loaded_model' for prediction
    if model_to_use is not None:
        sample_reshaped = np.array(sample).reshape(1, -1)
        # IMPORTANT: Based on your training flow, the scaler was NOT used
        # on the features input to the model. So, we don't scale here.
        prediction = model_to_use.predict(sample_reshaped)
        return prediction[0]
    else:
        print("Model not loaded. Cannot make prediction.")
        return None


# --- Example Usage ---
# To use the predict function, you need a sample with the correct features.
# This sample needs to be in the format the model expects, which is the 50
# selected numeric features followed by the entropy placeholder value.

# --- Creating a realistic example sample is HARD without knowing how
# --- the 50 features were selected and how entropy is derived for a new file.
# --- For demonstration, let's create a dummy sample with 51 random values
# --- This dummy sample format matches the shape of X_selected_with_entropy (1, 51)
dummy_sample = np.random.random(size=(51,)) # Assuming 50 selected + 1 entropy

# Make a prediction using the loaded model
if loaded_model is not None:
    predicted_class = predict_loaded_model(dummy_sample, loaded_model)

    if predicted_class is not None:
        print(f"\nPrediction for dummy sample: {predicted_class} ({'Malware' if predicted_class == 1 else 'Benign'})")

# Step 30: Test Prediction on Sample

# Ensure loaded_model is available from the previous step
# (where you loaded the model and defined predict_loaded_model)

if loaded_model is not None:
    # Select the first sample from the test set
    sample_to_test = X_test[0]

    # Use the predict_loaded_model function with the loaded model
    predicted_class_for_sample = predict_loaded_model(sample_to_test, loaded_model)

    if predicted_class_for_sample is not None:
        print(f"Predicted class for the first test sample: {predicted_class_for_sample} ({'Malware' if predicted_class_for_sample == 1 else 'Benign'})")
else:
    print("Cannot test prediction: Model was not loaded successfully in the previous step.")
